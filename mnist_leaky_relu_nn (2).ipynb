{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.4231, Accuracy: 0.1176\n",
      "Epoch 10, Loss: 0.8615, Accuracy: 0.8211\n",
      "Epoch 20, Loss: 0.6413, Accuracy: 0.8035\n",
      "Epoch 30, Loss: 0.5032, Accuracy: 0.8613\n",
      "Epoch 40, Loss: 0.4280, Accuracy: 0.8855\n",
      "Epoch 50, Loss: 0.3945, Accuracy: 0.8923\n",
      "Epoch 60, Loss: 0.3720, Accuracy: 0.8971\n",
      "Epoch 70, Loss: 0.3548, Accuracy: 0.9014\n",
      "Epoch 80, Loss: 0.3410, Accuracy: 0.9044\n",
      "Epoch 90, Loss: 0.3295, Accuracy: 0.9076\n",
      "Epoch 100, Loss: 0.3195, Accuracy: 0.9106\n",
      "Epoch 110, Loss: 0.3108, Accuracy: 0.9130\n",
      "Epoch 120, Loss: 0.3029, Accuracy: 0.9151\n",
      "Epoch 130, Loss: 0.2957, Accuracy: 0.9169\n",
      "Epoch 140, Loss: 0.2891, Accuracy: 0.9188\n",
      "Epoch 150, Loss: 0.2830, Accuracy: 0.9207\n",
      "Epoch 160, Loss: 0.2773, Accuracy: 0.9229\n",
      "Epoch 170, Loss: 0.2719, Accuracy: 0.9244\n",
      "Epoch 180, Loss: 0.2668, Accuracy: 0.9256\n",
      "Epoch 190, Loss: 0.2620, Accuracy: 0.9273\n",
      "Epoch 200, Loss: 0.2574, Accuracy: 0.9284\n",
      "Epoch 210, Loss: 0.2530, Accuracy: 0.9295\n",
      "Epoch 220, Loss: 0.2488, Accuracy: 0.9307\n",
      "Epoch 230, Loss: 0.2448, Accuracy: 0.9316\n",
      "Epoch 240, Loss: 0.2409, Accuracy: 0.9326\n",
      "Epoch 250, Loss: 0.2372, Accuracy: 0.9338\n",
      "Epoch 260, Loss: 0.2335, Accuracy: 0.9349\n",
      "Epoch 270, Loss: 0.2300, Accuracy: 0.9358\n",
      "Epoch 280, Loss: 0.2266, Accuracy: 0.9370\n",
      "Epoch 290, Loss: 0.2234, Accuracy: 0.9380\n",
      "Epoch 300, Loss: 0.2202, Accuracy: 0.9388\n",
      "Epoch 310, Loss: 0.2171, Accuracy: 0.9398\n",
      "Epoch 320, Loss: 0.2141, Accuracy: 0.9407\n",
      "Epoch 330, Loss: 0.2112, Accuracy: 0.9414\n",
      "Epoch 340, Loss: 0.2084, Accuracy: 0.9423\n",
      "Epoch 350, Loss: 0.2057, Accuracy: 0.9430\n",
      "Epoch 360, Loss: 0.2031, Accuracy: 0.9437\n",
      "Epoch 370, Loss: 0.2005, Accuracy: 0.9443\n",
      "Epoch 380, Loss: 0.1980, Accuracy: 0.9451\n",
      "Epoch 390, Loss: 0.1956, Accuracy: 0.9455\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Import NumPy for numerical operations\n",
    "from tensorflow.keras.datasets import mnist  # Import MNIST dataset from TensorFlow\n",
    "from tensorflow.keras.utils import to_categorical  # Import utility for one-hot encoding\n",
    "\n",
    "# Load MNIST dataset (handwritten digits 0-9)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1] for better convergence\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape input images from (28, 28) to (784,) vectors for the neural network\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "# Convert class labels (0-9) into one-hot encoded vectors\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Initialize weights and biases using He initialization for better training stability\n",
    "w1 = np.random.randn(28*28, 128) * np.sqrt(2 / (28*28))  # First layer weights\n",
    "b1 = np.zeros((1, 128))  # First layer biases\n",
    "w2 = np.random.randn(128, 10) * np.sqrt(2 / 128)  # Second layer weights\n",
    "b2 = np.zeros((1, 10))  # Second layer biases\n",
    "\n",
    "# Define hyperparameters for training\n",
    "epochs = 400  # Number of training iterations\n",
    "learning_rate = 0.3  # Step size for weight updates\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: Compute activations for the first hidden layer\n",
    "    z1 = x_train @ w1 + b1  # Linear transformation\n",
    "    a1 = np.maximum(0.004 * z1, z1)  # Leaky ReLU activation function\n",
    "\n",
    "    # Compute activations for the output layer\n",
    "    z2 = a1 @ w2 + b2  # Linear transformation for output layer\n",
    "    z2 -= np.max(z2, axis=1, keepdims=True)  # Prevent numerical overflow in softmax\n",
    "    y_pred = np.exp(z2) / np.sum(np.exp(z2), axis=1, keepdims=True)  # Softmax activation\n",
    "\n",
    "    # Compute categorical cross-entropy loss (1e-8 added for numerical stability)\n",
    "    loss = -np.sum(y_train * np.log(y_pred + 1e-8)) / y_train.shape[0]\n",
    "\n",
    "    # Backpropagation: Compute gradients for the output layer\n",
    "    dz2 = y_pred - y_train  # Gradient of loss w.r.t. output layer activation\n",
    "    dw2 = np.dot(a1.T, dz2) / y_train.shape[0]  # Gradient w.r.t. second layer weights\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True) / y_train.shape[0]  # Gradient w.r.t. second layer biases\n",
    "\n",
    "    # Compute gradients for the first hidden layer\n",
    "    dz1 = np.dot(dz2, w2.T) * ((z1 > 0) + 0.004 * (z1 <= 0))  # Leaky ReLU derivative\n",
    "    dw1 = np.dot(x_train.T, dz1) / x_train.shape[0]  # Gradient w.r.t. first layer weights\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True) / x_train.shape[0]  # Gradient w.r.t. first layer biases\n",
    "\n",
    "    # Update parameters using gradient descent\n",
    "    w1 -= learning_rate * dw1\n",
    "    b1 -= learning_rate * db1\n",
    "    w2 -= learning_rate * dw2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    # Print training progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)  # Convert softmax probabilities to class labels\n",
    "        y_true_labels = np.argmax(y_train, axis=1)  # Extract true labels\n",
    "        accuracy = np.mean(y_pred_labels == y_true_labels)  # Compute training accuracy\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.63%\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in range(10000):\n",
    "    sample = x_test[i].reshape(1, -1)  \n",
    "    z1 = sample @ w1 + b1\n",
    "    z1 = np.maximum(0.004 * z1, z1)\n",
    "    z2 = z1 @ w2 + b2\n",
    "    y_pred = np.exp(z2) / np.sum(np.exp(z2), axis=1, keepdims=True)\n",
    "    predicted_label = np.argmax(y_pred)\n",
    "    true_label = np.argmax(y_test[i])\n",
    "    \n",
    "    if predicted_label == true_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = (correct_predictions / 10000) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(x_test.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
